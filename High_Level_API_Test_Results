function create_dataset() Results

    Memory, adjusted for @profile:

    Line #    Mem usage    Increment   Line Contents
    ================================================
        37     57.7 MiB      0.0 MiB   @profile
        38                             def create_dataset():
        39     58.2 MiB      0.5 MiB       file = h5py.File(USED_PATH + str(random.getrandbits(8)) + ".hdf")
        40
        41                                 # Create standard, random dataset
        42     88.8 MiB     30.5 MiB       r_set  = np.random.randint(0, 1000, size=(200,200, 200), dtype='i')
        43     88.8 MiB      0.0 MiB       file.create_dataset("standard", (200,200, 200), dtype = 'i', data = r_set)
        44
        45                                 # float dataset
        46    149.8 MiB     61.0 MiB       f_set = np.random.rand(200,200, 200)
        47    149.8 MiB      0.0 MiB       file.create_dataset("float", (200,200, 200), dtype='f', data=f_set)
        48
        49                                 #Create New Group
        50    149.8 MiB      0.0 MiB       sub = file.create_group("subgroup")
        51                                 # chunked dataset
        52    180.3 MiB     30.5 MiB       c_set = np.random.randint(0, 1000, size=(200,200, 200), dtype='i')
        53    182.9 MiB      2.5 MiB       sub.create_dataset("chunked", (200,200, 200), dtype='i', data= c_set, chunks=(10,10,10))
        54
        55                                 #Create Nested Group
        56    182.9 MiB      0.0 MiB       nest = sub.create_group("nested")
        57
        58                                 # Compressed Dataset
        59    182.9 MiB      0.1 MiB       nest.create_dataset("compressed", (200,200, 200), dtype= 'i', data= r_set, compression="gzip", compression_opts=9)

    - Average Net Memory of 10 iterations: 182.6 MiB (191.5 MB)

    - Time Usage without line 59: as it contributes to 97.5% of time usage. 32.4s with and 0.6 s without compression

     Line #      Hits         Time  Per Hit   % Time  Line Contents
    ==============================================================
        38                                           def create_dataset():
        39         1         6170   6170.0      0.5      file = h5py.File(USED_PATH + str(random.getrandbits(8)) + ".hdf")
        40
        41                                               # Create standard, random dataset
        42         1       121350 121350.0      9.6      r_set  = np.random.randint(0, 1000, size=(200,200, 200), dtype='i')
        43         1       366710 366710.0     29.0      file.create_dataset("standard", (200,200, 200), dtype = 'i', data = r_set)
        44
        45                                               # float dataset
        46         1       228618 228618.0     18.1      f_set = np.random.rand(200,200, 200)
        47         1        72213  72213.0      5.7      file.create_dataset("float", (200,200, 200), dtype='f', data=f_set)
        48
        49                                               #Create New Group
        50         1          275    275.0      0.0      sub = file.create_group("subgroup")
        51                                               # chunked dataset
        52         1       109115 109115.0      8.6      c_set = np.random.randint(0, 1000, size=(200,200, 200), dtype='i')
        53         1       358512 358512.0     28.4      sub.create_dataset("chunked", (200,200, 200), dtype='i', data= c_set, chunks=(10,10,10))
        54
        55                                               #Create Nested Group
        56         1          268    268.0      0.0      nest = sub.create_group("nested")

function slicing_and_operating_datasets()

    Memory:
        Line #    Mem usage    Increment   Line Contents
    ================================================
        63     57.9 MiB      0.0 MiB   @profile
        64                             def slicing_and_operating_datasets(PATH):
        65     58.7 MiB      0.8 MiB       file = h5py.File(PATH)
        66
        67                                 #Slicing sequential data
        68     58.8 MiB      0.2 MiB       r_set= file.get("standard")
        69     59.0 MiB      0.2 MiB       slicer1 = r_set[0,0,:]
        70     59.0 MiB      0.0 MiB       slicer2 = r_set[0,:,0]
        71     59.1 MiB      0.0 MiB       slicer3 = r_set[:,0,0]
        72     59.1 MiB      0.0 MiB       slicer4 = r_set[:,1:4,1:4]
        73     59.1 MiB      0.0 MiB       slicer5 = r_set[1:4,1:4, :]
        74
        75                                 # Slicing compressed Data
        76     59.1 MiB      0.0 MiB       c_set = file.get("subgroup/nested/compressed")
        77     60.6 MiB      1.5 MiB       slicec1 = c_set[0, 0, :]
        78     61.1 MiB      0.6 MiB       slicec2 = c_set[0, :, 0]
        79     61.5 MiB      0.4 MiB       slicec3 = c_set[:, 0, 0]
        80     61.8 MiB      0.3 MiB       slicec4 = c_set[:, 1:4, 1:4]
        81     61.9 MiB      0.1 MiB       slicec5 = c_set[1:4, 1:4, :]
        82
        83                                 #Slicing chunked Data
        84     61.9 MiB      0.0 MiB       ch_set = file.get("subgroup/chunked")
        85     61.9 MiB      0.0 MiB       slicech1 = ch_set[0, 0, :]
        86     62.0 MiB      0.1 MiB       slicech2 = ch_set[0, :, 0]
        87     63.2 MiB      1.2 MiB       slicech3 = ch_set[:, 0, 0]
        88     63.2 MiB      0.0 MiB       slicech4 = ch_set[:, 1:4, 1:4]
        89     63.3 MiB      0.1 MiB       slicech5 = ch_set[1:4, 1:4, :]


    Time:
        Line #      Hits         Time  Per Hit   % Time  Line Contents
    ==============================================================
        64                                           def slicing_and_operating_datasets(PATH):
        65         1         2972   2972.0      2.1      file = h5py.File(PATH)
        66
        67                                               #Slicing sequential data
        68         1          657    657.0      0.5      r_set= file.get("standard")
        69         1         1178   1178.0      0.8      slicer1 = r_set[0,0,:]
        70         1          976    976.0      0.7      slicer2 = r_set[0,:,0]
        71         1        18822  18822.0     13.2      slicer3 = r_set[:,0,0]
        72         1        23776  23776.0     16.6      slicer4 = r_set[:,1:4,1:4]
        73         1          889    889.0      0.6      slicer5 = r_set[1:4,1:4, :]
        74
        75                                               # Slicing compressed Data
        76         1          848    848.0      0.6      c_set = file.get("subgroup/nested/compressed")
        77         1        12051  12051.0      8.4      slicec1 = c_set[0, 0, :]
        78         1        19900  19900.0     13.9      slicec2 = c_set[0, :, 0]
        79         1        20832  20832.0     14.6      slicec3 = c_set[:, 0, 0]
        80         1        18631  18631.0     13.0      slicec4 = c_set[:, 1:4, 1:4]
        81         1        10269  10269.0      7.2      slicec5 = c_set[1:4, 1:4, :]
        82
        83                                               #Slicing chunked Data
        84         1          553    553.0      0.4      ch_set = file.get("subgroup/chunked")
        85         1         1652   1652.0      1.2      slicech1 = ch_set[0, 0, :]
        86         1         2600   2600.0      1.8      slicech2 = ch_set[0, :, 0]
        87         1         4540   4540.0      3.2      slicech3 = ch_set[:, 0, 0]
        88         1          986    986.0      0.7      slicech4 = ch_set[:, 1:4, 1:4]
        89         1          841    841.0      0.6      slicech5 = ch_set[1:4, 1:4, :]

    - Shows the use of chunking in situations is effective in reducing times
    - Compression increases read times
    - Compressing increases time cost significantly, provides no major memory improvements